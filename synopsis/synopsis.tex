\documentclass[12pt,a4paper,twocolumn]{article}
\usepackage{times} % times font
\usepackage{mathptmx} % times font in maths
\usepackage[top=0.65in, bottom=0.65in, left=0.65in, right=0.65in]{geometry}
\usepackage{multirow} %in tables
\usepackage{caption} % in tables
\pagenumbering{gobble}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\usepackage[pdftex]{graphicx}
\usepackage{lipsum}
\usepackage{amsmath}

% \usepackage{hyperref}
% \usepackage{subfigure}
% \usepackage{indentfirst} % indent frst paragraph of section
% \usepackage[usenames,dvipsnames]{color}
\newcommand{\ts}{\textsuperscript}

\begin{document}

\twocolumn[
\begin{@twocolumnfalse}
\begin{center}
	\begin{large}
	{\HRule \\[0.2cm]}
	\textsc{Instance-wise thresholding methods for multi-label classification}
	{\HRule \\[0.3cm]}
	\end{large}

	\begin{minipage}{ 0.44\textwidth }
		\begin{flushleft}
			\textit{Author:}\\
			Kacper \textbf{Sokol}
		\end{flushleft}
	\end{minipage}
	\begin{minipage}{ 0.44\textwidth }
		\begin{flushright}
			{\textit{Supervisors:}\\
			Peter \textbf{Flach}, Meelis \textbf{Kull}\\[0.3cm]}
		\end{flushright}
	\end{minipage}
\end{center}
\end{@twocolumnfalse}
]

\section*{\texttt{Review}}
This work will cover instance-wise thresholding on multi-label ranking and scoring. To be more precise, we will examine choosing a threshold based on particular data instance features on ordered(according to their relevance) list of labels.\\
We are going to evaluate performance of such task measured on high cardinality \emph{delicious}($19.020$) and average cardinality \emph{yeast}($4.237$), \emph{mediamill}($4.376$) datasets.\\ % Additionally we will draw the histogram of of labels per instance to visualise distributions of labels.
To train the classifier and rank corresponding test instances we will use: \emph{MEKA}, \emph{Mulan} and independent implementation of \emph{Probabilistic Classifiers Chains}.\\
As evaluation measures we plan to employ example based measures: \emph{recall}, \emph{precision}, \emph{$F_\beta$}, \emph{accuracy}, and \emph{hamming loss}; and label based measures: \emph{macro-averaging} and \emph{micro-averaging}.%; ranking measures: \emph{one-error}, \emph{coverage}, \emph{ranking loss} and \emph{average precision}; finally we will also use \emph{hierarchical-loss}.\\

\section*{\texttt{Multi-label ranking}}
Multi-label ranking is used to order the labels according to their relevance given particular instance. Then the thresholding algorithm is applied to allow selecting relevant number of labels per instance.\\
For our experiment we will use the following ranking and scoring approaches:
\begin{enumerate}
\item Rankings:
	\begin{itemize}
	\item Multi-label perceptron. %* ?????
	\item Ranking by pairwise comparison. %*
	% \item Calibrated label ranking. %*
	\item Ad-aBoost.MR. %*
	\end{itemize}
\item Scores:
	\begin{itemize}
	\item Random k-Labelsets a.k.a.\ RAkEL---label power-set. %*
	% \item Ensembles of pruned set---label power-set.
	\item Ensembles of classifier chains---pruned sets i.e.\ extension of label power-set which reduces cardinality. % ?????
	\item Some probabilistic classifiers.???
	\end{itemize}
\end{enumerate}

For the approaches based on binary or multi-class solutions we aim to test them with: \emph{k-NN}, \emph{SVM} and \emph{decision trees}.

\section*{\texttt{Data driven thresholding}}
We aim at applying the following data dependent thresholding methods:
\begin{enumerate}
\item MetaLabeler---regression from the feature vector to the number of labels per instance based on:
	\begin{itemize} % 05670068.pdf
	\item Original input space.
	\item Score vectors.
	\item Sorted score vectors.
	\end{itemize}
	and:
	\begin{itemize} % 05670068.pdf
	\item Regression.
	\item Multi-class classification.
	\end{itemize}

\item Artificial label for pairwise comparisons---PCC, EPCC, etc.
\item Artificial label to extract threshold value.
% \item Score per label\ts{*}. % assume we give the score of the label corresponding to relative frequency of this label according to data point and then cut at given fixed threshold---maybe
\item Threshold prediction---based on $t(x)$ minimizing:
	$$
	| \lambda_j \in Y : s_j(x) \leq t(x) | + | \lambda_j \in \Delta \text{\textbackslash{}} Y : s_j(x) \geq t(x) |
	$$
\end{enumerate}

\emph{Original input space} was discovered to be optimal[ref.] therefore in our experiment we will only use this one out of three listed.\\

\section*{\texttt{Empirical comparison}}
Finally we aim to ``compare and contrast'' acquired results according to aforementioned criteria. Our goal is also to indicate winning thresholding algorithm if possible and discover any dependencies that may arise between type of dataset, manner of fixing the threshold, and ranking approach.

\section*{\texttt{Goal}}
We aim at comparing 2\ts{nd} and 3\ts{rd} thresholding method against each other as they are very simmilar and may vary only a little.\\
We will also compare all fancy thresholding methods against one simple global threshold(like 0.5).\\


% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/figure4b.png}
% % \begin{tiny}
% \caption{Simulation of two interconnected neurons with inhibitory synapses.\label{fig:part4b}}
% % \end{tiny}
% \vspace{0.2cm}
% \end{figure}

\end{document}

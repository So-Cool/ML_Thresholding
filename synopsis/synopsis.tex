\documentclass[12pt,a4paper,twocolumn]{article}
\usepackage{times} % times font
\usepackage{mathptmx} % times font in maths
\usepackage[top=0.7in, bottom=0.7in, left=0.7in, right=0.7in]{geometry}
\usepackage{multirow} %in tables
\usepackage{caption} % in tables
\pagenumbering{gobble}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\usepackage[pdftex]{graphicx}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{cite} % BiTeX

% \usepackage{hyperref}
% \usepackage{subfigure}
% \usepackage{indentfirst} % indent frst paragraph of section
% \usepackage[usenames,dvipsnames]{color}
\newcommand{\ts}{\textsuperscript}

\begin{document}

\twocolumn[
\begin{@twocolumnfalse}
\begin{center}
	\begin{large}
	{\HRule \\[0.2cm]}
	\textsc{Instance-wise thresholding methods for multi-label classification}
	{\HRule \\[0.3cm]}
	\end{large}

	\begin{minipage}{ 0.44\textwidth }
		\begin{flushleft}
			\textit{Author:}\\
			Kacper \textbf{Sokol}
		\end{flushleft}
	\end{minipage}
	\begin{minipage}{ 0.44\textwidth }
		\begin{flushright}
			{\textit{Supervisors:}\\
			Peter \textbf{Flach}, Meelis \textbf{Kull}\\[0.3cm]}
		\end{flushright}
	\end{minipage}

	~\\ \textbf{Organise experiments to create sort of ToDo list!}

\end{center}
\end{@twocolumnfalse}
]

\section*{\texttt{Review}}
This work will cover instance-wise thresholding on multi-label ranking and scoring. To be more precise, we will examine choosing a threshold based on particular data instance features on ordered(according to their relevance) list of labels.\\
We are going to evaluate performance of such task measured on high cardinality: \emph{delicious}($19.020$), \emph{CAL500}($26.044$), and average cardinality: \emph{yeast}($4.237$) and \emph{mediamill}($4.376$), \emph{enron}($3.378$) datasets.\\ % Additionally we will draw the histogram of of labels per instance to visualise distributions of labels.
To train the classifiers and rank corresponding test instances we will use: \emph{MEKA} and \emph{Mulan}.\\% and independent implementation of \emph{Probabilistic Classifiers Chains}.\\
As evaluation measures~\cite{DBLP:reference/dmkdh/TsoumakasKV10} we plan to employ example based measures such as: \emph{exact match ratio}, \emph{accuracy}, \emph{precision}, \emph{recall}, \emph{$F_1$ measure}, \emph{subset 0--1 loss} and \emph{hamming loss}.

%; and label based measures: \emph{macro-averaging} and \emph{micro-averaging}.\\
%; ranking measures: \emph{one-error}, \emph{coverage}, \emph{ranking loss} and \emph{average precision}; finally we will also use \emph{hierarchical-loss}.\\

\section*{\texttt{Multi-label ranking}}
Multi-label ranking is used to order the labels according to their relevance given particular instance. On the other hand, scoring algorithms output score per label for each instance thus they facilitate labels ordering.\\
Next step is to apply thresholding algorithm to select relevant number of labels per instance.\\
For our experiment we will use the following ranking and scoring approaches:

\begin{enumerate}
\item Rankings:
	\begin{itemize}
	\item Multi-label perceptron~\cite{Crammer03afamily}. This approach produces one perceptron per label with weight updated in such a way that each perceptron leads to perfect labels ranking. %* ?????
	\item Ranking by pairwise comparison~\cite{Wu:2004:PEM:1005332.1016791}. This model learns pairwise binary models(for all possible pairs). Scores are obtained by accumulating votes for each label. %*
	% \item Calibrated label ranking. %*
	\item AdaBoost.MR~\cite{Schapire00boostexter:a}---adaptation of AdaBoost algorithm for multi-label datasets which tries to find hypothesis which places correct labels at the top of the ranking. %*
	\end{itemize}
\item Scores:
	\begin{itemize}
	\item Random k-Labelsets a.k.a.\ RAkEL (label power-set family)~\cite{Tsoumakas:2007:RKE:1421665.1421705}. It learns several multi-class classifiers on random subsets of the original set of labels where classes are distinct label combinations. It produces a score vector by averaging zero-one predictions for multiple multi-label models. %*
	% \item Ensembles of pruned set---label power-set.
	\item Ensembles of classifier chains~\cite{Read:2009:CCM:1617459.1617477}. Each chain link is constructed as a binary model that predict a label given feature vector extended with values of the labels that preceded in sequence (eg.\ PCC, EPCC, etc.). % ?????
	% \item Probabilistic ...\
	\end{itemize}
\end{enumerate}

For the approaches based on binary or multi-class solutions we aim to test them with: \emph{k-NN}, \emph{SVM} and \emph{decision trees} algorithms.

\section*{\texttt{Data driven thresholding}}
We aim at applying the following data dependent thresholding methods:


\begin{enumerate}
\item MetaLabeler~\cite{Tang:2009:LSM:1526709.1526738}---mapping from the feature vector to the number of labels per instance based on:\\

	different input spaces
	\begin{itemize} % 05670068.pdf
	\item Original input space---not altered in any way.
	\item Score vectors $\phi(x_i)$(score based)---$f_j(x)$ is a score prediction for particular label.
	$$
	\phi(x_i) = [ f_1(x_i), f_2(x_i), ...\ , f_K(x_i) ]
	$$
	\item Sorted score vectors $\hat{\phi}(x_i)$(rank based).
	$$
	\hat{\phi}(x_i) = \texttt{sort} ( f_1(x_i), f_2(x_i), ...\ , f_K(x_i) )
	$$
	\end{itemize}

	and:
	\begin{itemize} % 05670068.pdf
	\item Regression~\cite{conf/ictai/IoannouSTV10}. Perform a regression from features vector to the number of labels per instance.
	\item Multi-class classification~\cite{conf/ictai/IoannouSTV10}. Perform a classification from features vector to the classes represented by number of labels per instance.
	\end{itemize}

\item Threshold prediction~\cite{Elisseeff01akernel}---based on $t(x)$ minimizing:
	$$
	t(x_i) = \text{argmin}_t $$ $$ | \lambda_j \in Y : s_j(x_i) \leq t | + | \lambda_j \in \Delta \text{\textbackslash{}} Y : s_j(x_i) \geq t |
	$$

\item Artificial label for threshold extraction. This method uses the concept of pseudo-label but instead of treating it as a cut point the threshold is acquired by extracting the label counter (this method can only be used with pairwise ranking approach).% and using it as a threshold.
% \item Score per label\ts{*}. % assume we give the score of the label corresponding to relative frequency of this label according to data point and then cut at given fixed threshold---maybe

\item Artificial label for calibrated label ranking~\cite{DBLP:conf/ecml/ParkF07}. This method introduces an artificial label with labels applying to the instance being above it and all the rest of labels being below. It uses this label as a calibration factor and once classification is performed as a threshold (this method can only be used with pairwise ranking approach).

\end{enumerate}

Out of the top three MetaLabeler approaches the \emph{original input space} was discovered to be optimal. We will use this fact to restrict our experiment pool, therefore we will neither test \emph{score vectors} nor \emph{sorted score vectors}.\\



\section*{\texttt{Deliverables}}
Finally, we aim to ``compare and contrast'' acquired results according to aforementioned criteria. Our goal is to indicate winning data driven thresholding algorithm if possible and discover any dependencies that may arise between characteristic of a dataset, threshold approach, and chosen classification algorithm.\\

The main purpose of this experiment is to examine whether advanced, data dependent threshold selection outperforms simple, global thresholding---basic benchmark will be produced. Moreover, study will be done to check whether increase in computational complexity will significantly improve labeling and in what cases advanced approach is worth the slowdown.\\

We also aim at comparing 2\ts{nd} and 3\ts{rd} thresholding method against each other as they are very similar and their results may be hard to distinguish on the general test. The purpose of this subtask is to see whether introduced artificial label is necessary or it is enough to treat it as a truncation point.\\


\bibliography{ref}{}
\bibliographystyle{plain}
% http://mulan.sourceforge.net/doc/

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/figure4b.png}
% % \begin{tiny}
% \caption{Simulation of two interconnected neurons with inhibitory synapses.\label{fig:part4b}}
% % \end{tiny}
% \vspace{0.2cm}
% \end{figure}

\end{document}
